{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lst_py import lst_urls\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_g = []\n",
    "lst_c = []\n",
    "lst_err = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for u in range(len(lst_urls)):\n",
    "for u in tqdm(range(len(lst_urls))):\n",
    "\n",
    "    try:\n",
    "        # URL que deseas extraer\n",
    "        url = lst_urls[u]\n",
    "\n",
    "        # Realizar la solicitud HTTP para obtener el contenido de la página\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "\n",
    "            url_data = {}\n",
    "            url_conse = {}\n",
    "\n",
    "            url_data[\"url\"] = url\n",
    "            \n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # FECHAS\n",
    "            # ### tbl_dates = soup.select(\"table.table.table_summary\")[n_tables[len(tbl_dates)]].select_one(\"tbody\").select(\"tr\")\n",
    "            tbl_dates = soup.select(\"table.table.table_summary\")\n",
    "            \n",
    "            l = None\n",
    "\n",
    "            \n",
    "            for i in range(len(tbl_dates)):\n",
    "                if \"Fecha de vencimiento\" in f'{tbl_dates[i].encode_contents()}':\n",
    "                    l = i\n",
    "            \n",
    "            tbl_dates = tbl_dates[l].select_one(\"tbody\").select(\"tr\")\n",
    "            \n",
    "            \n",
    "            for i in range(len(tbl_dates)):\n",
    "                index = 0 if i == 0 else 1\n",
    "                key = \"duracion_years\" if i == 0 else tbl_dates[i].select(\"td\")[0].text.strip().replace(\" \",\"_\")\n",
    "                value =  tbl_dates[i].select(\"td\")[index].text.strip().replace(\" \",\"_\").strip()\n",
    "                value = \"SIN INFORMACIÓN\" if len(value) == 0 else value\n",
    "                url_data[key] = value\n",
    "            # -------------------------------------------------------\n",
    "            # CONSECIONES\n",
    "            tbl_eml = soup.find(\"table\", {\"id\": \"tblCoberturas\"})\n",
    "            \n",
    "            if tbl_eml:\n",
    "                thead = tbl_eml.select(\"thead\")[0].select(\"tr\")[0].select(\"th\")\n",
    "                tbody = tbl_eml.select(\"tbody\")[0].select(\"tr\")\n",
    "                for tr in range(len(tbody)):\n",
    "                    html_tr = tbody[tr].select(\"td\")\n",
    "                    l_html_tr = len(html_tr)\n",
    "                    temp_dict = {}\n",
    "                    for l in range(l_html_tr):\n",
    "                        e = thead[l].text\n",
    "                        d = html_tr[l].text.strip()\n",
    "                        d = \"SIN INFORMACIÓN\" if len(d) == 0 else d\n",
    "                        temp_dict[\"url\"] = url\n",
    "                        temp_dict[e] = d\n",
    "\n",
    "                    lst_c.append(temp_dict)\n",
    "            \n",
    "            # FRECUENCIA PRINCIPAL\n",
    "            # tbl_frec = soup.find(\"table\", {\"id\": \"tblCoberturas\"})\n",
    "              \n",
    "            lst_g.append(url_data)\n",
    "            lst_c.append(url_conse)\n",
    "        \n",
    "    except Exception:\n",
    "        lst_err.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = pd.DataFrame(lst_g)\n",
    "df_f.to_csv('./resultados_csv/fechas.csv', index=False)\n",
    "\n",
    "df_c = pd.DataFrame(lst_c)\n",
    "df_c.to_csv('./resultados_csv/cobertura.csv', index=False)\n",
    "\n",
    "df_e = pd.DataFrame(lst_err, columns=[\"url_con_error\"])\n",
    "df_e.to_csv(\"./resultados_csv/errores_urls.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
